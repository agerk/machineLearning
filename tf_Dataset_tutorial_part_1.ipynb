{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_Dataset_tutorial_part-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/agerk/machineLearning/blob/master/tf_Dataset_tutorial_part_1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vTF7a5DcbSID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "ed2e1da0-3c4b-4498-877e-3cefd1b3c3cc"
      },
      "cell_type": "code",
      "source": [
        "# Install latest version of tensorflow\n",
        "!pip install tensorflow==1.8.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 49.1MB 989kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.9.0,>=1.8.0 (from tensorflow==1.8.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.1MB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.6.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.14.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (3.5.2.post1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0) (0.31.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (2.6.11)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (1.5.0)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0) (0.9999999)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0) (39.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Found existing installation: tensorboard 1.7.0\n",
            "    Uninstalling tensorboard-1.7.0:\n",
            "      Successfully uninstalled tensorboard-1.7.0\n",
            "  Found existing installation: tensorflow 1.7.0\n",
            "    Uninstalling tensorflow-1.7.0:\n",
            "      Successfully uninstalled tensorflow-1.7.0\n",
            "Successfully installed tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "glTMZOoQd1dp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Install Google Drive</h3>"
      ]
    },
    {
      "metadata": {
        "id": "6y3-73lYd07Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGVLxS-bb1yq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Intro to TensorfFow Dataset and Estimator</h2>\n",
        "\n",
        "- this tutorial follows the the tutorial series on tf.Dataset and tf.Estimator by Google\n",
        "- example model is the Iris flower classification problem"
      ]
    },
    {
      "metadata": {
        "id": "NMxFW8VMfiFr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c81567f4-9bff-4701-ef58-9ac4da0271ab"
      },
      "cell_type": "code",
      "source": [
        "# Load liberary\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O46KoGsXdelB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Download dataset</h3>"
      ]
    },
    {
      "metadata": {
        "id": "3VFFIhsXcbfB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "# Get the file from Drive\n",
        "\n",
        "# get id of train  and data from Google Drive\n",
        "train = drive.CreateFile({'id':'1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo'})   # replace the id with id of file you want to access\n",
        "train.GetContentFile('iris_training.csv')  \n",
        "\n",
        "test = drive.CreateFile({'id':'1ThMeLns4SyPwZTPivUOheM7nKq5RF2AI'})   # replace the id with id of file you want to access\n",
        "test.GetContentFile('iris_test.csv')  \n",
        "\n",
        "#3. Read file as panda dataframe\n",
        "# iris_training = pd.read_csv('iris_training.csv')\n",
        "# iris_test = pd.read_csv('iris_test.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0CCuTB7cdpC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e47f848f-c5f7-4e27-da09-2958c3837901"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleDriveFile({'id': '1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo', 'kind': 'drive#file', 'etag': '\"qu1qQsDCUNdGLg7FCvF6ozYbuj8/MTUyNjgzOTI2Nzk5NA\"', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo', 'webContentLink': 'https://drive.google.com/uc?id=1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo&export=download', 'alternateLink': 'https://drive.google.com/file/d/1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo/view?usp=drivesdk', 'embedLink': 'https://drive.google.com/file/d/1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo/preview?usp=drivesdk', 'iconLink': 'https://drive-thirdparty.googleusercontent.com/16/type/text/csv', 'title': 'iris_training.csv', 'mimeType': 'text/csv', 'labels': {'starred': False, 'hidden': False, 'trashed': False, 'restricted': False, 'viewed': True}, 'createdDate': '2018-05-20T17:31:06.235Z', 'modifiedDate': '2018-05-20T18:01:07.994Z', 'modifiedByMeDate': '2018-05-20T18:01:07.994Z', 'lastViewedByMeDate': '2018-05-20T17:31:06.235Z', 'markedViewedByMeDate': '1970-01-01T00:00:00.000Z', 'version': '4', 'parents': [{'kind': 'drive#parentReference', 'id': '11-8FECUGwrvayJ-7Fp_YQhaHAdBHl4Bd', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo/parents/11-8FECUGwrvayJ-7Fp_YQhaHAdBHl4Bd', 'parentLink': 'https://www.googleapis.com/drive/v2/files/11-8FECUGwrvayJ-7Fp_YQhaHAdBHl4Bd', 'isRoot': False}], 'downloadUrl': 'https://doc-0g-8s-docs.googleusercontent.com/docs/securesc/5276k71irm1d3pj2l8s6dgo3agtii7mp/kfeuj813a85r67mn5pa79783nrd6qc0e/1526911200000/04556457425278207801/04556457425278207801/1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo?e=download&gd=true', 'userPermission': {'kind': 'drive#permission', 'etag': '\"qu1qQsDCUNdGLg7FCvF6ozYbuj8/hVRmMIgLCR93NVC1SBozSdX9UNU\"', 'id': 'me', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1da4BRxdb5CoAsTs2omowTLQnZGEVhvjo/permissions/me', 'role': 'owner', 'type': 'user'}, 'originalFilename': 'iris_training.csv', 'fileExtension': 'csv', 'md5Checksum': 'b522e13521ffde008d6b2c94a89a508b', 'fileSize': '2194', 'quotaBytesUsed': '2194', 'ownerNames': ['Agerneh Dagnew'], 'owners': [{'kind': 'drive#user', 'displayName': 'Agerneh Dagnew', 'isAuthenticatedUser': True, 'permissionId': '04556457425278207801', 'emailAddress': 'agerdagnew@gmail.com'}], 'lastModifyingUserName': 'Agerneh Dagnew', 'lastModifyingUser': {'kind': 'drive#user', 'displayName': 'Agerneh Dagnew', 'isAuthenticatedUser': True, 'permissionId': '04556457425278207801', 'emailAddress': 'agerdagnew@gmail.com'}, 'capabilities': {'canCopy': True, 'canEdit': True}, 'editable': True, 'copyable': True, 'writersCanShare': True, 'shared': True, 'explicitlyTrashed': False, 'appDataContents': False, 'headRevisionId': '0B6d00we8_TbqUXl4SkFCbDJBMkJ2U3NoT3gydWtVRHVaV0s4PQ', 'spaces': ['drive']})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "ijCAv_G2uYeC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Create features for Deep Neural Network Classifier</h2>\n",
        "\n",
        "Model Atchitecture:\n",
        "\n",
        "1.  Input to model \n",
        "  - Sepal length\n",
        "  - Sepal width\n",
        "  - Petal length\n",
        "  - Petal width\n",
        "  - lable (0 for Setosa, 1 for Versicolor, and 2 for Virginica)\n",
        "\n",
        "1.   Two hidden layers with 10 neurons each\n",
        "2.   Outpit prediction is probability of Iris type\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "21EpxGATwR8E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Preprocessing using tf.Dataset</h2>\n",
        "\n",
        "Its the new way of creating input pipeline for Tensorflow-- better performance than f*eed_dict *and *queue-based* pipelines"
      ]
    },
    {
      "metadata": {
        "id": "YDHnhLmgwybx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Dataset**: Consist of the follwoing subclasses \n",
        "1.  TextLineDataset: reads lines from text files\n",
        "1.  TFRecordDataset: reads records from disc\n",
        "2.   FixedLengthRecordDataset: reads fixed size records from binary files\n",
        "\n",
        "and **Iterator** instance: provides access to dataset elemenst during train, eval and serving. "
      ]
    },
    {
      "metadata": {
        "id": "vMAmSJseK29m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1zolqGjULNKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80c4eee9-511f-4e97-c540-6cccf820bb97"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  iris_test.csv  iris_training.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I7nDazWtdtnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Iris dataset\n",
        "# it has 120 example\n",
        "# 4 features -- sepal length and width, petal length and width\n",
        "# target variale-- flower type (setosa, versicolor and virginica)\n",
        "\n",
        "# Fetch and store dataset\n",
        "PATH = \"drive/\"\n",
        "# PATH_DATASET = PATH + os.sep + dataset\n",
        "FILE_TRAIN = \"iris_training.csv\"\n",
        "FILE_TEST = \"iris_test.csv\"\n",
        "# URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "# URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "\n",
        "# def download_dataset(url, file):\n",
        "#   if not os.path.exists(PATH_DATASET):\n",
        "#     os.mkdirs(PATH_DATASET)\n",
        "#   if not os.path.exist(file):\n",
        "#     data = request.urlopen(url).read()\n",
        "#     with open(file, \"wb\") as f:\n",
        "#       f.write(data)\n",
        "#       f.close()\n",
        "# download_dataset(URL_TRAIN, FILE_TRAIN)\n",
        "# download_dataset(URL_TRAIN, FILE_TRAIN)\n",
        "\n",
        "# Data representation\n",
        "feature_names = [\n",
        "    'SepalLength',\n",
        "    'SepalWidth',\n",
        "    'PetalLength',\n",
        "    'PetalWidth']\n",
        "\n",
        "# for training, we will need our a function that reads raw input files and reurn feature and label data.\n",
        "# tf.Estimator requires a function which can accomplish that\n",
        "\n",
        "# implimentation of data pipeline using Dataset API\n",
        "def decode_csv(line):\n",
        "  \"\"\"\n",
        "  Args: a line of sample from csv\n",
        "  Returns: dict with features and label\n",
        "  \"\"\"\n",
        "  parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])    # split each line in the dataset. Lable should be integer [0]\n",
        "  label = parsed_line[-1]    # last element is the flower label (class) \n",
        "  del parsed_line[-1]    # delete last elment\n",
        "  features = parsed_line    # Everythinf minus label\n",
        "  return dict(zip(feature_names, features)), label\n",
        "\n",
        "def input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
        "  \"\"\"\n",
        "  Args: Input file for train or test, shffle, num of times to iterate over the records. \n",
        "        for example if repeat_count=1, then record is read only once\n",
        "  \n",
        "  Returns: list of features and batch labels for bactch training  \n",
        "  \"\"\"\n",
        "  dataset = (tf.data.TextLineDataset(file_path)    #read text file \n",
        "            .skip(1)    # skip column header\n",
        "            .map(decode_csv))    # Apply element-wise transformation \n",
        "#   print(dataset)\n",
        "  if perform_shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=256)    # randomize dataset order using a window of 256 elements\n",
        "  dataset = dataset.repeat(repeat_count)    # repeat dataset numrepear_count times\n",
        "  dataset = dataset.batch(32)    # read 32 sample randomly\n",
        "  iterator = dataset.make_one_shot_iterator()    # iterat throught a batch\n",
        "  batch_features, batch_labels = iterator.get_next()    # get feature and labels \n",
        "  return batch_features, batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vq0KLuk5hUMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3b416620-8d84-40b6-b286-5f1086594c90"
      },
      "cell_type": "code",
      "source": [
        "# print out first batch\n",
        "next_batch = input_fn(FILE_TRAIN, True)\n",
        "# retrieving and printing one batch of data\n",
        "with tf.Session() as sess:\n",
        "  first_batch = sess.run(next_batch)\n",
        "print(first_batch)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n",
            "({'SepalLength': array([7. , 5.7, 5.1, 5.3, 5. , 4.6, 5.1, 4.8, 6.8, 6. , 6.7, 5.6, 5. ,\n",
            "       4.9, 4.9, 7.2, 6.2, 6.9, 7.7, 5.7, 6.5, 5.2, 5. , 5.9, 6.9, 7.9,\n",
            "       4.6, 6.3, 4.8, 5. , 5.4, 4.9], dtype=float32), 'SepalWidth': array([3.2, 3.8, 3.5, 3.7, 3. , 3.4, 3.8, 3.4, 3. , 2.7, 3.1, 2.5, 3.5,\n",
            "       2.4, 3. , 3. , 3.4, 3.1, 2.6, 4.4, 3. , 2.7, 3.4, 3.2, 3.1, 3.8,\n",
            "       3.1, 2.5, 3. , 3.3, 3. , 3.1], dtype=float32), 'PetalLength': array([4.7, 1.7, 1.4, 1.5, 1.6, 1.4, 1.5, 1.6, 5.5, 5.1, 5.6, 3.9, 1.6,\n",
            "       3.3, 1.4, 5.8, 5.4, 5.1, 6.9, 1.5, 5.2, 3.9, 1.5, 4.8, 4.9, 6.4,\n",
            "       1.5, 5. , 1.4, 1.4, 4.5, 1.5], dtype=float32), 'PetalWidth': array([1.4, 0.3, 0.3, 0.2, 0.2, 0.3, 0.3, 0.2, 2.1, 1.6, 2.4, 1.1, 0.6,\n",
            "       1. , 0.2, 1.6, 2.3, 2.3, 2.3, 0.4, 2. , 1.4, 0.2, 1.8, 1.5, 2. ,\n",
            "       0.2, 1.9, 0.3, 0.2, 1.5, 0.1], dtype=float32)}, array([1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 0, 2, 2, 2, 2, 0, 2, 1,\n",
            "       0, 1, 1, 2, 0, 2, 0, 0, 1, 0], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Be9-4NTR0xk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> tf.Estimator</h2>\n",
        "\n",
        "High level API that reduces boler plate codes that were necessary to transform data pipeline to tensorflow\n",
        "\n",
        "It has two possible ways to build model using **Estimator**\n",
        "- Pre-made Estimator:  canned estimator for specific type of model. It includes\n",
        " - DNN classifier and \n",
        " - DNN regressor\n",
        "- Estimator base class: customemizable estimator"
      ]
    },
    {
      "metadata": {
        "id": "AQQCv6OBbbVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Model training and evaluation </h3>\n",
        "\n",
        "- more on tf.feature_column https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"
      ]
    },
    {
      "metadata": {
        "id": "pHHVoOt6hU7U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "d87c0ef9-f09a-4399-9894-10070b8d36ea"
      },
      "cell_type": "code",
      "source": [
        "# Convert feature_columns to numeric columns which specifies input to the mode\n",
        "# feature column serve as a bridge between input data and model \n",
        "# All features are numeric-- use tf.feature_numeric.numeric_columm use defual dtype-- tf.float32\n",
        "# Defual numeric column creates a single value(scalar)\n",
        "feature_columns = [tf.feature_column.numeric_column(col) for col in feature_names]\n",
        "\n",
        "# Create a deep neural regression classifier using DNNClassifier pre-made estimator\n",
        "model = tf.estimator.DNNClassifier(\n",
        "     feature_columns=feature_columns,\n",
        "     hidden_units = [10, 10],  # two layers with 10 neurons\n",
        "     n_classes = 3,\n",
        "     model_dir=PATH)   # path to checkpoints\n",
        "\n",
        "# Train model for 10 iterations\n",
        "model.train(input_fn = lambda: input_fn(FILE_TRAIN, True, 10))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'drive/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb1c47f2e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1 into drive/model.ckpt.\n",
            "INFO:tensorflow:loss = 75.18382, step = 1\n",
            "INFO:tensorflow:Saving checkpoints for 38 into drive/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 13.682814.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7fb1bcd632b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "Ifuw4R-XPn2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "b3ec49fb-6bb4-439a-97b1-7720fcbe4aaf"
      },
      "cell_type": "code",
      "source": [
        "# Model evaluation \n",
        "# run model against test data\n",
        "# return will contain evaluation metrics: accuracy, average loss\n",
        "model_eval = model.evaluate(\n",
        "  input_fn = lambda: input_fn(FILE_TEST, False, 4))\n",
        "print(\"Evaluation results\")\n",
        "for key in model_eval:\n",
        "  print(\"  {} is: {}\".format(key, model_eval[key]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-05-21-15:45:16\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/model.ckpt-38\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2018-05-21-15:45:17\n",
            "INFO:tensorflow:Saving dict for global step 38: accuracy = 0.3, average_loss = 1.0589244, global_step = 38, loss = 31.767735\n",
            "Evaluation results\n",
            "  accuracy is: 0.30000001192092896\n",
            "  average_loss is: 1.0589244365692139\n",
            "  loss is: 31.76773452758789\n",
            "  global_step is: 38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpNvQXEsc_7s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>Making Predictions Using Our Trained Model</h3>"
      ]
    },
    {
      "metadata": {
        "id": "7JHDG4YsZw2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "9fdd25c8-d99e-4db5-ca73-cc95f0aa550e"
      },
      "cell_type": "code",
      "source": [
        "# predict the type of Iris flowers\n",
        "prediction_iris = model.predict(input_fn=lambda: input_fn(FILE_TEST, False, 1))\n",
        "print(\"prediction on test data\")\n",
        "for prediction in prediction_iris:\n",
        "  # will print predicted class, i.e: 0, 1, or 2 if preiction is \n",
        "  # Iris Setosa, Vericolor, Virginica\n",
        "  print(prediction[\"class_ids\"][0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction on test data\n",
            "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/model.ckpt-38\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xFtMvgPQfKaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Making prediction on data in memory</h3>\n",
        "  \n",
        " this does not change the model configuration, but dataset API to use a memory structure"
      ]
    },
    {
      "metadata": {
        "id": "HC6ZZ00vdO8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5cdd3b2e-4a2f-4c0e-c982-9bfb07699f97"
      },
      "cell_type": "code",
      "source": [
        "# create a dataset for prediction\n",
        "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
        "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
        "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
        "\n",
        "def new_input_fn():\n",
        "  def decode(x):\n",
        "    x = tf.split(x, 4)    # split into four feature columns\n",
        "    return dict(zip(feature_names, x))    # build dictionary of features\n",
        "  \n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
        "  dataset = dataset.map(decode)\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  next_feature_batch = iterator.get_next()\n",
        "  return next_feature_batch, None   # in predition no label, since we are trying to predict labels based on input\n",
        "\n",
        "# predicti Iris flower type of all our prediction_input \n",
        "predict_memory = model.predict(input_fn=new_input_fn)\n",
        "\n",
        "# print results\n",
        "print(\"Prediction on memory\")\n",
        "for idx, prediction in enumerate(predict_memory):\n",
        "  type = prediction[\"class_ids\"][0]   # get predicted class index\n",
        "  if type == 0:\n",
        "    print(\" I think: {}, is Iris Setosa\".format(prediction_input[idx]))\n",
        "  if type == 1:\n",
        "    print(\" I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
        "  if type == 0:\n",
        "    print(\" I think: {}, is Iris Virginica\".format(prediction_input[idx]))\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction on memory\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from drive/model.ckpt-38\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A5DhJIhLiUOi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*This tutorial is inspired by this blog *https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html\n",
        "\n",
        "I strongly recomend to read through to better understand the mechanics of tf.Dataset"
      ]
    }
  ]
}